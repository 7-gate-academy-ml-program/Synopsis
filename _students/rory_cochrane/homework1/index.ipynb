{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of homework1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B42YV4QyuRKZ",
        "colab_type": "text"
      },
      "source": [
        "# Instructions:\n",
        "## Please save a copy of this notebook to your google drive and answer the questions below.\n",
        "## Once completed please submit your notebook to the following [GitHub Repo](https://github.com/7-gate-academy-ml-program/Synopsis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8mmOkNLyoWt",
        "colab_type": "text"
      },
      "source": [
        "### Name: Rory Cochrane\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBTTGCUO_mJD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdkchPgStYin",
        "colab_type": "text"
      },
      "source": [
        "# 1.)  What is the difference between Classification and Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aKWEz_atmEp",
        "colab_type": "text"
      },
      "source": [
        "Classifiaction seeks to answer a question with a discrete set of possible answers, whereas regression seeks to place an answer along a continuum of possible answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma7t_7yBtmud",
        "colab_type": "text"
      },
      "source": [
        "# 2.) What is the Curse of Dimensionality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C3_8rvyyDYA",
        "colab_type": "text"
      },
      "source": [
        "The curse of dimensionailty is that our intuitions about the world around us fail when dealing with high dimensional spaces. For example: \n",
        "\n",
        "-Removing 1/10th of a line leaves us with 90% of the length\n",
        "\n",
        "-Removing 1/10 of the radius of a circle leaves us with 81% of the area\n",
        "\n",
        "-Removing 1/10 of the radius of a sphere leaves us with 72.9% of the volume\n",
        "\n",
        "By the time we remove 1/10th of the radius of a 25 dimensional sphere, we've removed over 90% of the 'volume'.\n",
        "\n",
        "This is all important to keep in mind when dealing with high dimensional data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_rFydWVyDl3",
        "colab_type": "text"
      },
      "source": [
        "# 3.) What is Cross Validation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-iA3jnCtxqP",
        "colab_type": "text"
      },
      "source": [
        "Typically, when training a machine learning model, one must separate data into testing and training sets. When very little data is available it can be difficult to sacrifice some of it to the testing set. \n",
        "\n",
        "Cross validation offers a solution to this problem. By separating the data into 'folds', and then training several times, each time with a different fold as the test set, we can make sure our model gets to use as much data to train as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieaSLJE2tyVm",
        "colab_type": "text"
      },
      "source": [
        "# 4.) On a high level how do Decision  Trees work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD6nBEWCt8ud",
        "colab_type": "text"
      },
      "source": [
        "A decision tree is essentially a directed acyclic graph. At each node of the graph a decision must be made, and the outcome of the decision determines which node of the graph you will travel to next. The final nodes of the graph represent the final decisons, each arrived at by making smaller decisions as you traverse the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6akxVwD_t82G",
        "colab_type": "text"
      },
      "source": [
        "# 5.) In regards to SVMs what is the Kernel Trick?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGR-lhHuKNq",
        "colab_type": "text"
      },
      "source": [
        "When two or more groups of data do not appear to be easily separable in a linear way, we can still separate them by plotting them in higher dimensions. \n",
        "\n",
        "For example, consider dataset A, clustered at the centre of a 2-d (x,y) plot, and dataset B, spread out in a circle of radius 1. The 2 datasets are clearly separate, but we can't draw a straight line to separate them. \n",
        "\n",
        "Using the kernel trick, we move them to a 3-d space, with the relationship: $$z =\\sqrt{x^2+y^2}$$\n",
        "\n",
        "Now, along the z-axis, A will be clustered around z=0, and B will be clustered around z=1. Therefore, the datasets are clearly separable by a plane at roughly z=0.5."
      ]
    }
  ]
}